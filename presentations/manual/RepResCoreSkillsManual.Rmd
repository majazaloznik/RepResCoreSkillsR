---
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    includes:
      in_header: latex/header.tex
      before_body: latex/before_body.tex
      after_body: latex/after_body.tex 
---
```{r setup, echo=FALSE, message=FALSE, warning = FALSE}
library(knitr)
library(xlsx)
library(foreign)
library(dplyr)
library(tidyr)
library(plotrix)
# opts_knit$set(root.dir = "~/Dropbox/XtraWork/R stuff/RepResCoreSkillsR")
opts_knit$set(root.dir = "C:/Users/sfos0247/Dropbox/XtraWork/R stuff/RepResCoreSkillsR")
opts_chunk$set(warning=FALSE, message=FALSE)
options(scipen=999, digits=4)
```
\newpage


# Reproducible Research 

Reproducible research means making the data and the code of our analysis available in a way that is sufficient and easy for an independent researcher to recreate our findings. 

This is the golden standard of scientific inquiry, and is increasingly and rightly becoming a requirement in academic publishing, and by funding bodies. 

It is also a way of establishing better working habits, reducing the potential for error, developing a more streamlined research process, and making for easier collaboration. 

Reproducible research does take a bit of upfront investment in learning the tools and setting up your workflow. Luckily RStudio has integrated many of the tools required in one platform, making it easier than ever to apply the principles of reproducibility consistently and comprehensively. 

This practical course will focus in particular on how to set-up an RStudio project and associate file and folder structure, and get you on the right track towards *literate programming*. We will then cover a complete workflow structure including downloading and importing data, *tidying* it up, using some basic programming structures to improve your code, and finally the `dplyr` package, which is already revolutionising data manipulation in R by providing a comprehensive set of tools that follow a very intuitive logic.

There is plenty more that cannot be covered in a 3 hour course. In particular we will not discuss version control (e.g. github) and the *knitting* of text and analysis, or their publishing on-line directly from RStudio on to RPubs. You will be able to see the results of these practices in the way the very course materials are prepared, and they can be accessed on-line in a dedicated repository public github repository: [http://tinyurl.com/RCSRepRes](http://tinyurl.com/RCSRepRes). 

For an excellent and in-depth source on all of this and more, see Christopher Gandrud's book on Reproducible Research in R and RStudio, also in a public github repository: [https://github.com/christophergandrud/Rep-Res-Book]( https://github.com/christophergandrud/Rep-Res-Book) and for your convenience an old compiled copy of his first edition can be also be found in this course's repository in the `literature` folder. 



\newpage
#  Set-up 

R can be run from the *console*, by typing in commands at the command prompt. The level of *reproducibility* of this approach is approximately zero. Writing code into an R script---a text file with the extension `.R`---and executing it from a file is the recommended minimum level of reproducibility for any analysis using the R programming language. Going one step further is to use `R projects`, a way of integrating your R scripts with added functionality that is integrated in the RStudio work environment, and it is this set-up that we will be adopting here. 


## RStudio
In the last few years RStudio has beyond a doubt become the most popular integrated development environment (IDE) for R, and rightly so. It is open source and cross-platform.  In addition to allowing easy project management and integrated version control, it also provides all the tools for dynamically creating *knitted* documents and directly publishing them on-line. The RStudio crew are also active developers of interactive graphical tools and new developments are constantly being added to an already excellent toolbox. If you prefer to use another environment you should however still be able to apply all of the principles of reproducible research covered here and beyond, although it might take a little bit more work. 

Instructions for installing R and RStudio on your laptop:

First install R

1. For Windows [instructions here: https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
2. For Mac [instructins here: https://cran.r-project.org/bin/macosx/](https://cran.r-project.org/bin/macosx/)
3. For Linux [instructions here : (https://cran.r-project.org/bin/linux/ubuntu/README](https://cran.r-project.org/bin/linux/ubuntu/README)

Then install RStudio - Desktop edition - by selecting the appropriate installer for your operating system from [this link: https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/). 


## Project management

A crucial requirement for conducting reproducible research, and one that has to be carefully considered before you embark on your analysis, is your plan on how the data, code and outputs will be organised. The project management structure proposed here is just a suggestion, and you should adapt it to your specific needs, but it is highly recommended that you stick to one such system consistently, instead of coming up with 'ad hoc ' solutions for every new project. 


### Project folder structure

RStudio makes it extremely easy to divide your work into separate projects, allowing you to neatly organize and access your work. This means assigning a single folder for each project, and within that folder organizing your work into sub-folders. Depending on your type of project this may vary, but a good starting point would be something along the lines of the following project sub folders:

- *data* - holds all the raw data files
- *scripts* - holds all the R code, preferable split into smaller, more modular code files
- *figures* - to store outputs of your data analysis, or external figures to be used in reports
- *outputs* - presentations, reports etc. that are compiled dynamically within the project


### Project files

The folder structure maps onto the following classification of different types of files that will most often be involved in your project:

1. R project level files, are files created by R or RStudio for each project:

- `.Rproj` - *the project file*
- `.RData` - *the workspace file*: contains all the objects in your workspace. It is advisable as a general rule that you do not use `.RData` files at all in your work! 
- `.Rhistory` - *the history file* logs your command history
- `.Rprofile` - *the settings file* where you can add commands to automatically rune every time you open a specific project. 

2. R script files: These are the executable files with R code. The `.R` extension are pure coding files, `.Rmd` files are ones that combine code and text. 

3. Data files: can come in a variety of formats, `.csv`, `.xls`, `.dat` etc.. Best practice is to programatically (download and) import the files into R and never manipulate the raw data directly!

4. Figures can be produced by R in various formats from `.jpeg` and `.png` to vector graphics for example `.eps`, although some details will vary according to your platform. 

5. Outputs and presentations: many types of reports can be automatically produced directly in R, including `.pdf`, `.doc` and `.html` files, which can also be published on-line directly. 


## Human readability
One of the often overlooked principles underlying reproducible research is trying to ensure the human readability of as much of your files as possible. This applies to the types of data and other file formats used, as well as your coding style. Making them human readable is one way of trying to future-proof your work. 

### File Formats
Try to avoid binary file formats. This includes e.g. `.xls` files, but also R's native workspace file, which is saved with an  `.RData` extension. Text, delimited or comma separated values (`.csv`) are safe formats that are easily transferable, human readable, and relatively future proof. 

### Consistent coding style e.g.:

None of this will matter though, if you do not *document everything* you do! This means not running any code from the command prompt, but instead always writing it into a script file (`.R` or `.Rmd`) and running it from there. 

Another key element of human readability is trying to keep to a consistent coding style. This is not always easy, but it pays to get into some good habits while it's still early. Two excellent starting points are

* [Google's R Style Guide: https://google.github.io/styleguide/Rguide.xml](https://google.github.io/styleguide/Rguide.xml)
* [Hadley Wickham's Style Guide: http://adv-r.had.co.nz/Style.html](http://adv-r.had.co.nz/Style.html)

You do not have to follow these to the letter (the two are not consistent anyway) - but they should give you an idea about what are some things you might want to establish a rule for yourself. At least for the sake of the future you, who will one day have to re-read your messy, uncommented code. 

### Commenting 

The importance of commenting your code can never be understated. Some programmers advocate  *self documenting code* -- that is code that is self-explanatory and does not require comments --  and this may be worthy a goal to aspire to, but in the mean time: comment, comment, and comment some more. 

![Code Quality (https://xkcd.com/1695/)](../../figures/code_quality_2.png)



\newpage
# Workflow
> *Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.* 

Source: NY Times^[[http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)]

## Importing data

Regardless of whether your data is stored locally or downloaded from the web, you should never manipulate the original data directly. This is crucial for the integrity of your reproducible research process. 

R has utilities for importing data from a wide variety of sources including proprietary formats. Ideally you want to be working with .csv files, as they are the cleanest and easiest to import, but often you have no choice in the matter. 

In the practical we will therefore import `.csv`, `.xls` and `.sav` files, including downloading and unzipping them. Here is a list of some common formats^[For a more comprehensive list of possible input formats see this tutorial: [https://www.datacamp.com/community/tutorials/r-data-import-tutorial](https://www.datacamp.com/community/tutorials/r-data-import-tutorial).] and the packages used for importing them, refer to the help pages for more details:

* comma separated values -- `read.csv()` 
* tab-delimited text file -- `read.table()`
* other delimited files -- `read.delim()`
* Minitab -- `read.mtb()` from  `library(foreign)`
* SPSS -- `read.spss()` from `library(foreign)`
* Stata -- `read.dta()` from  `library(foreign)`
* Excel -- `read.xls()` from `library(gdata)`
* Excel -- `loadWorkbook()` from `library(XLConnect)`

The basic import functions of the `read.table()` family all have a `nrows` argument, which is particularly useful if you do not know the structure of the data and are dealing with a large fine. In which case it is recommended you try a test import with e.g. `nrows=10`, and check the result before attempting to import the full file. 




We will store all our data files in the `data` folder of our project, from where they will be imported into R. This means the original files remain *untouched* by the data analysis and should never be overwritten as the result of your analysis. 

While you might find it easier to simply download a file into your folder, this poses the problem of loosing track of where the data was sourced from. It is therefore highly recommended you download the data programmatically (i.e. in the script) if possible, and if not, that you use comments within the code to describe the source of the files: their origin and the date accessed. We will see an example in the practical. 


## Data tidying

> *Tidy datasets are all alike but every messy dataset is messy in its own way.* â€“ Hadley Wickham

A great deal of data tidying can be done manually with the base R functions. Additionally there are several packages available with more specific functions. In this course we will use the `tidyr` package by Hadley Wickham, which is particularly well integrated with the `dplyr` package we will be using in the second part of this course. 

The underlying principle of the `tidyr` package is *tidy data*, which must satisfy the following three principles:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Source: H. Wickham (2014) *Tidy Data* (available:  [http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf))

This may seem trivial, but it is in fact common to encounter data that does not conform to these principles. The four workhorse functions of `tidyr` that should solve all your data tidying needs are:

* `spread()`
* `gather()`
* `separate()` 
* `unite()`


### `spread()`

Below we can see an example of a *messy* table: it is messy because each observation is in fact represented in two rows. The third column is not really a variable, but contains variable names or *keys* (`density` and `population`), while the fourth column contains their values. 

```{r, echo=FALSE}
messy.02 <- read.table("data/demo/messy.02.txt")
messy.02
```

The syntax for `spread()` takes the following form:

`spread(data, key, value)`

The *key--value* pair is the underlying logic of the tidy data table. We can decompose the data into a collection of key--value pairs such as this:

Key : Value
```{r, eval=FALSE}
Country: Norway
Country: Slovenia 
Country: UK

Year: 2010
Year: 2050

Population: 4891300
Population: 2003136
...

Density: 16.07489
Density: 20.91484
...

```

In a tidy data table each cell contains a *value* and the *keys* are the column names. So in order to tidy up this table we need the `var.name` values to become the column names, so that is our *key* and we need the var.value values to become the values in the new columns, so we designate that as the *value* argument:


```{r, echo=c(3:4)}
library(tidyr)
library(dplyr)
tidy.02 <- spread(messy.02, key= var.name, value = var.value)
```

Resulting in the tidy table:
```{r}
tidy.02
```



### `gather()`

Here is another messy table:
```{r, echo=2}
messy.01 <- read.table("data/demo/messy.01.txt")
messy.01
```

Now we have three variables: the country, which is in the first column, the year, which is across the header row (representing the *keys*), and the population (representing the *values*), which is in the second and third columns. Using `gather()` we can tidy up the table, so that now each of the three variables has its own column, and each row is an observation:

The syntax for `gather()` takes the following form:

`gather(data, key, value, ...)`

where the `...` represents the columns we want to gather, in our case columns 2 and 3. The key and value arguments are the *names* of the two new variables, or columns we are creating: the *key* is currently in the column names of columns two and three - so we want it to become `year`, and the *values* are in the cells of those two columns, so we want it to become `population`.  


```{r, echo=c(3,5)}
library(tidyr)
library(dplyr)
tidy.01 <- gather(messy.01, year, population, 2:3)
tidy.01 <-  mutate(tidy.01, year=ifelse(year=="X2010", 2010, 2050))
tidy.01
```


### `separate()` and `unite()` 

Separate and unite are straightforward helper functions for the reshaping done by gather and spread. The following table for example requires spreading, but the `double.key` variable contains both *values* (years) and  *keys* (population and density):

```{r, echo=FALSE}
messy.03 <- read.table("data/demo/messy.03.txt")
messy.03
```

These are separated simply by the following code into `year` and `key`, which can then be used to reshape the table as we did above. 

```{r}
tidy.03 <- separate(messy.03, double.key, c("year", "key"))
tidy.03
```

The function `unite` is the inverse of `separate`, and merges the values of selected columns into a new single column. In both cases you can change the separator using the `sep=` argument. 
```{r}
messy.again <- unite(tidy.03, new.double.key, key, year,  sep = " in the year ")
messy.again
```

For an excellent write-up of the main `tidyr` functions see Garrett Grolemund's post here [http://garrettgman.github.io/tidying/](http://garrettgman.github.io/tidying/).

For a quick tidyr cheat-sheet stick this to your wall: [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), also available in the `literature` folder of this course's repository. 


# PRACTICAL 1: Project management and Data Tidying {-}

In this first practical we will: 

(i) Start a new project and organize the folder structure
(ii) Download and import our data.
(iii) Tidy up the data. 

In particular if you are working on a University computer and not on your own laptop, the first and second part might be slightly problematic due to permissions. This manual gives you instructions on how to set everything up on you own laptop, but during the course it will be quicker if you download a working setup from the course repository. 


## P1.i Start a new project and organize the folder structure {-}

### Download the course repository {-}

The complete documentation for this course is available as an RStudio project in a public github repository: [https://github.com/majazaloznik/RepResCoreSkillsR](https://github.com/majazaloznik/RepResCoreSkillsR) or for extra convenience: [http://tinyurl.com/RCSRepRes](http://tinyurl.com/RCSRepRes). 



![Click the green button and sellect `Download zip`](../../figures/clone_repo.png)
Follow the link to the repository and then click on the green `Clone or download` button and select `Download ZIP`, save the file to the computer and un-zip the folder. Now open the rpoject file:  `RepResCoreSkillsR.Rproj` in RStudio. You can now skip to part (ii) of this practical on importing and cleaning the data. 


### Create new RStudio project {-}

1. Open RStudio
2. Select the project menu in the top right-hand corner and select `New Project`
3. Select New Directory and choose the name of your project (e.g. RRepResCourse)^[As a general rule you should avoid spaces in file and folder names, although you will probably be fine if you ignore this advice.].
4. RStudio has now created a new project file in your folder, which you can see in the Files pane.
5. Run the command `getwd()` in the console. You should note that RStudio has automatically set the working directory in the top level of your project folder. 
6. Click on the project menu again and select Project Options. On the options "Restore .RData to the workspace at start-up" and "Save workspace to .RData on exit" select No. In fact, you should set that as a global option (Tools/Global Options) -- for truly reproducible research you should never have to load a previous workspace!

### Folder and File Structure {-}

1. Using the New Folder button in the Files pane, create three folders called (some equivalent of)
  * data
  * scripts
  * figures
  * presentations
2. For the rest of this practical, all you need is to download the file `pop2010.csv` from the github repo into your `data` folder. 
* Downloading single files from github can be cumbersome - you need to navigate to the file [https://github.com/majazaloznik/RepResCoreSkillsR/blob/master/data/pop2010.csv](https://github.com/majazaloznik/RepResCoreSkillsR/blob/master/data/pop2010.csv) and right-click on the `Download` button on the top right and select `Save link as`. 
3. You can also download this manual in pdf format to your `presentation` folder if you wish. 
4. Create an .RProfile file using the command `file.edit(".Rprofile")`. This will open a new script tab for you to edit. The content of the .RProfile gets automatically run every time you open your project. It is therefore a good idea to e.g. load any packages you will need from your .RProfile, instead of doing it manually every time. 

For this practical you will need the following packages (type this into your `.RProfile` file and save).
```{r, eval = FALSE}
library(xlsx)
library(foreign)
library(tidyverse)
library(plotrix)
```

If any of the packages are not installed on your computer you must first run `install.packages("<name of package>")`. Now save and close the `.RProfile` file. In fact, close RStudio completely, and try opening the project again. You should see the messages indicating that the packages from the `.RProfile` file have have been loaded automatically.  

## P1.ii Import and clean some data {-}

### Comma separated values {-}

First create a new `.R` script file in your `scripts` folder or equivalent via the drop-down menu or using `Ctrl+Shift+N`. Naming  it something like `01-DataImport.R` will make your project management easier in the long run, but feel free to set up your own file naming system -- but try to stick with it!

It is good practice to establish a header system for all your script files, such as the one below. The `#` lines are also a good way of making the code file structure easy to understand. Following the [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml) rule "The maximum line length is 80 characters.", a nice little trick is to make these separators 80 hashtags long, which gives you a nice visual reference for when your code gets too wide. 

```{r}
###############################################################################
## DATA IMPORT AND CLEANUP
###############################################################################
## 1.1  Import a .csv file
## 1.2  Download and import an Excel file
## 1.3  Download, unzip and import a .dat file 
###############################################################################

```

Let's see where our working directory is using `getwd()`. It should be in the main project folder. Now we are going to use the `read.csv()` function to import the data from the `pop2010.csv` file that we have in the `data` folder. But just to be safe we will first do a test run importing only 10 rows from the table, so we can inspect the result before importing the whole table:
```{r }
## 1.1  Import a .csv file
###############################################################################
getwd()

# test run
population2010 <- read.csv("data/pop2010.csv", nrows=10)
population2010
```

That looks good, the only thing is I can already tell you that all the data in this file is from 2010, so we do not really need the last column. We could delete it later, but we can also *not import it* in the first place: in order to skip it during import, we use the `colClasses` argument, and setting the seventh argument to `NULL`. 

```{r}
# don't forget to note the source of the data! e.g:
# downloaded manually from https://github.com/majazaloznik/RepResCoreSkillsR/
# blob/master/data/pop2010.csv?raw=true" on 20.6.2016
# MZ got it from:
# http://www.census.gov/data/developers/data-sets/international-database.html

# import full table except for 7th column (year)
population2010 <- read.csv("data/pop2010.csv", 
                           colClasses = c(rep(NA, 6), "NULL")) # skip "time"
# check how it looks
head(population2010)
tail(population2010)
```

Comma separated values (`.csv`) is one of the preferred formats to import data from, but R allows you to import from a variety of other formats, although this can sometimes get a bit more messy. 

### Excel files {-}

If you are working on your own laptop and have no problems running `library(xlsx)`, then you can continue with the instructions here. Otherwise simply run the following line and continue to part (iii) on tidying the data. 

```{r, eval = FALSE}
load("data/economic.situation.RData")
```

This time we will also first download the file, before importing a table from one of the spreadsheets. This is from [https://data.gov.uk/dataset/social_trends](https://data.gov.uk/dataset/social_trends), part of the government's open data access initiative and a great resource!

The first step is to use R to download the file from a url:
```{r}
## 1.2  Download and import an Excel file
###############################################################################
# url of the .xls file we want I've given it a tinyurl, but the real one is:
# http://www.ons.gov.uk/ons/rel/social-trends-rd/social-trends/
# social-trends-41/income-and-wealth-data.xls
data.url <- "http://tinyurl.com/Excel2106"

# download location 
data.location <-  paste( "data", "income-and-wealth-data.xls", sep = "/")

# download - Excel files are binary, so set the mode to "wb"!!
download.file(data.url, data.location, mode="wb")
```

You can now have a look in the data folder to check the file has been correctly downloaded and inspect it in Excel. We will import the table from the third worksheet, named "Table 1", on people's perceptions of the current economic situation. Close the Excel file before proceeding! Several solutions are available for importing Excel files into R, a nice overview can be found  [on this page: http://www.r-bloggers.com/read-excel-files-from-r/](http://www.r-bloggers.com/read-excel-files-from-r/). In this practical we will use the `xlsx` package:

```{r}
## Importing the data from an .xls file
library(xlsx)

# let's see what happens if we import the whole sheet
economic.situation <- read.xlsx(data.location, sheetIndex = 3)
```
Have a look at `economic.situation`. ^[Are you getting an error? That may be because you still have the Excel file open!]  It is not ideal, empty rows and columns are imported, as is the text at the top and the bottom of the worksheet. Luckily, `read.xlsx` has plenty of arguments that allow us to specify more precisely what we want to import. In this case, we can go one step further, and note that there are actually three separate tables in this worksheet, so it might be easiest to import them separately:

```{r}
## using rowIndex and colIndex select each subtable individually:
world.situation <- read.xlsx(data.location, sheetIndex = 3,
                         rowIndex=c(4, 6:8), colIndex = c(1:7))

UK.situation <- read.xlsx(data.location, sheetIndex = 3,
                             rowIndex=c(4, 11:13), colIndex = c(1:7))

household.situation <- read.xlsx(data.location, sheetIndex = 3,
                          rowIndex=c(4, 16:18), colIndex = c(1:7))
```

```{r, echo = FALSE}
save(world.situation, UK.situation, household.situation, file = "data/economic.situation.RData")
```


### Unzipping an SPSS file  {-}

As the final example of data import, another common occurrence is that we need to extract the data from a zipped file. This can also be done directly form R ^[This should work even if no winzip utility is installed on the machine?]. And to try out another data format we will import an SPSS file this time.^[The data file is supplementary material to the SPSS Survival Manual from a survey designed to explore the factors that impact on respondents' psychological adjustment and well-being. ] 


```{r}
## 1.3  Download, unzip and import a .dat file 
###############################################################################

data.zip.url <- paste0("http://spss.allenandunwin.com.s3-website-ap-southeast-2.",
                       "amazonaws.com/Files/survey.zip")

temp <- tempfile()
download.file(data.zip.url, temp)

# check what is in the zip file using list (doesn't extract anything)
unzip(temp, list=TRUE)

# Only one file, that's the one we want to extract to the data folder
unzip(temp, "survey.sav", exdir = "data")
unlink(temp)
```

You can now check the `data` folder and you should find the `survery.sav` file there. Even if you don't have SPSS installed on your computer, you can now open it using R (courtesy of the `foreign` package):

```{r}
# import the data as a data frame:
data.location <- paste("data","survey.sav", sep="/")
ed.psy.survey <- read.spss(data.location,  to.data.frame=TRUE)
# check what it looks like
ed.psy.survey[1:5,1:5]
```

Now we have the data but before we continue we'll just do a bit of housekeeping and clear our workspace of the objects we don't need any more (including the survey dataset, which we will not use in this practical)

```{r}
# CLEAN UP!
rm(economic.situation, ed.psy.survey, data.location, data.url, data.zip.url, temp)
```

## P1.iii  Data Tidying {-}

In the third part of this practical we will use the functions from the `tidyr` package to tidy up the two datasets. At this point your environment should have four tables: 

- `population2010`
- `household.situation`
- `UK.situation`
- `world.situation`

```{r, warning=FALSE, message=FALSE}
## 2.1 tidy up the population data
###############################################################################
library(tidyr)
```

The `population2010` data frame is already pretty tidy! The only issue with it is the `SEX` variable, which is coded for men (`SEX==1`), women (`SEX==2`), and both (`SEX==0`). We really only need to remove the rows with the values for `(SEX==0)`, but we can use this opportunity to perform a data check as well, while practising the `spread()` and `gather()` functions:

First let's try out our technique on a small subset of the data - this is good practice in general, especially if you are dealing with large datasets. We'll select only the observations for Aruba, and have a look at them:
```{r}
# try out our technique on a smaller subset of the data
test.data <- population2010[population2010$FIPS == "AA", ]
head(test.data)
```

We want to reshape the table so that the values of `SEX` will become new column names (i.e. *keys*), and that the *values* for these new keys will be the values from the variable `POP`. This means the `spread()` functions should look like this:

```{r}
tidy.test <- spread(test.data, SEX, POP )
head(tidy.test)
# and for clarity, let's rename the columns:
colnames(tidy.test)[5:7] <- c("both", "male", "female")

```

We can now check if the totals for men and women actually match, before we discard the column with the sum of both: 
```{r}
# calculate sum of males and females
tidy.test$check <- tidy.test$male + tidy.test$female

# compare it with the values already in the table:
all.equal(tidy.test$both, tidy.test$check)

# looks good, now we can remove both total columns:
tidy.test$check <- NULL
tidy.test$both <- NULL

# so now we have:
head(tidy.test)
```

And finally we have to use `gather()` to get back to a tidy table. Remember, with gather you need to pass the *names* of the new variables that are now the *key* and the *value*, and the column names which hold them:
```{r}
tidy.test <- gather(tidy.test, sex, population, 5:6)
# and let's check it again:
head(tidy.test)
```
If you are happy with the test run, you can now try it on the whole `population2010` table. The resulting table should look like this: 

```{r, echo=FALSE}
library(tidyr)
# SEX is the key:
tidy.population2010 <- spread(population2010, SEX, POP)

# for clarity, let's rename the columns:
colnames(tidy.population2010)[5:7] <- c("both", "male", "female")

# Now check if the sums are right, by creating a new column:
tidy.population2010$check <- tidy.population2010$male + tidy.population2010$female
# all.equal(tidy.population2010$both, tidy.population2010$check)

# looks good, now we can remove both total columns:
tidy.population2010$check <- NULL
tidy.population2010$both <- NULL

# now tidy it up again:
tidy.population2010 <- gather(tidy.population2010, sex, population, 5:6)
```

```{r}
head(tidy.population2010)
```

Most of the time you will not be lucky enough to work with as nicely formed datasets as the this population one. But the same tools can be used to disentangle much more messy tables, such as the ones we extracted from the Excel file before. 

Let's have a look at one of the three tables, e.g. `household.situation`, and see how it could be tidied up. What are the variables (that should be in the columns), and what are the observations (that should have one row each)? 
```{r}
## 2.2 tidy up the perception data
###############################################################################
household.situation
# first let's remane the column names
colnames(household.situation) <- c("perception", "inc.lt.20", 
                                   "inc.20.to.39", "inc.40.to.59", 
                                   "inc.60.to.99", "inc.gt.100", "all")
household.situation
```
 
In fact the whole table needs to be transposed, so that each population group represents one observation, and the proportion answering each question are the variables. In order to do that we need to first gather the data in long form, before spreading it out again wide.^[The `t()` function will transpose a data frame in R, try it out to see if it is a useful alternative to gather and spread. ] 

```{r}
# transpose using gather and spread  
X.household.situation <- gather(household.situation, income.group, proportion, 2:7)
tidy.household.situation <- spread(X.household.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.household.situation) <- c("income.group", "bad", "good", "neutral")
# check the result and remove the temporary table
tidy.household.situation
rm(X.household.situation)
```

Don't forget, we have two more tables, one for each perception question. Think about how you could merge all three tables into one tidy table? One option is to add a new variable that tells us what the perception is referring to:

```{r}
tidy.household.situation$perception <- "HH"
```
Then we can repeat the same process on the other two tables, before merging all three together using `rbind()`


```{r, echo=FALSE}
## 2.2.2. UK SITUATION
# first let's remane the column names
colnames(UK.situation) <- c("perception", "inc.lt.20", 
                                   "inc.20.to.39", "inc.40.to.59", 
                                   "inc.60.to.99", "inc.gt.100", "all")
# transpose using gather and spread  
X.UK.situation <- gather(UK.situation, income.group, proportion, 2:7)
tidy.UK.situation <- spread(X.UK.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.UK.situation) <- c("income.group", "bad", "good", "neutral")

# add variable for the question asked
tidy.UK.situation$perception <- "UK"

## 2.2.3. WORLD SITUATION
# first let's remane the column names
colnames(world.situation) <- c("perception", "inc.lt.20", 
                            "inc.20.to.39", "inc.40.to.59", 
                            "inc.60.to.99", "inc.gt.100", "all")
# transpose using gather and spread  
X.world.situation <- gather(world.situation, income.group, proportion, 2:7)
tidy.world.situation <- spread(X.world.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.world.situation) <- c("income.group", "bad", "good", "neutral")

# add variable for the question asked
tidy.world.situation$perception <- "W"
```
```{r}
# merge all the tables together
tidy.economic.situation <- rbind(tidy.household.situation,
                                 tidy.UK.situation,
                                 tidy.world.situation) 
tidy.economic.situation
```

Finally, you can now clear your workspace using `rm()` as we did before, to remove everything except for `tidy.population2010` and `tidy.economic.situation`. 

\newpage
# Efficient Coding
This section covers some of the most important skills to improve the efficiency, readability, and reproducibility of your R code. The standard control of the flow of your code that can be achieved with `ifelse` statements and looping is covered briefly, however you are encouraged in particular to explore the advantages of *vectorised* R code in particular the `apply` family of functions. Writing your own functions will greatly streamline your work, as well as forcing you to think in more abstract terms about your analysis - making it easily transferable and reproducible as opposed to limited to the specific situation you find yourself analysing at the moment.



## Standard control structures

An indispensable gain in efficiency of your programming can be achieved by using *control strucures* to control the execution of your code. These can be divided into *conditional execution* structures (`if` and `else` type functions) and *looping* structures. However, as we shall see in the next section, there are some some pretty good ways (and reasons) to avoid looping in R.

### Conditional execution

The standard syntax for conditional execution is as follows:

```{r, eval=FALSE}
if (condition) {
  # do something
} else {
  # do something else
}
```

In fact, you may also use only the `if()` construct on it's own:

```{r, eval=FALSE}
if (condition) {
  # do something
}
```

The if/else syntax also works in a single line, where you can dispense with the curly braces:

```{r, eval=FALSE}
if (x >= 0)  print("Poz") else print("Neg")
```

While this is more compact, it can impact readability, and can also make your code more difficult to debug and extend. Using curly braces and indenting the code properly will make it clearer to the reader, and also easier to e.g. extend via nesting:

```{r, eval=FALSE}
x <- runif(1) # randum number from uniform distribution [0,1]
if (x >= 0.6) {
  print("Good")
} else {
  if (x <= 0.4) {
    print("Bad")
  } else {
    print("Not Sure")}
}
```

The conditions to be evaluated are: 
```{r, eval=FALSE}
x == y   # x is equal to y
x != y   # x is not equal to y
x > y    # x is greater than y
x < y    # x is less than y
x <= y   # x is less than or equal to y
x >= y   # x is greater than or equal to y
x %in% y # x can be found in y
TRUE     # 
FALSE    #
```

And these can further be combined using standard logical operators:

```{r, eval=FALSE}
! x       # NOT
x & y     # AND
x | y     # OR
xor(x, y) # exclusive OR
```

What if you want to run a conditional statement over an entire vector? You might be tempted to jump to the next section on looping, and construct a loop going over each element of the vector and evaluating the condition. This would of course work, but it would be a very inefficient way of coding, and would not be taking advantage of the efficiencies of vectorisation in R: vectorised functions apply to whole vectors at once instead of evaluating for each element individually. We should therefore use the  *vectorised* form of the if/else construct `ifelse()`: 

```{r, eval=FALSE}
ifelse(condition, yes, no)
```

Here `yes` is the value to be returned if the condition is satisfied, and `no` if not. Similarly as above, `ifelse()` statements can also be nested as in this example:

```{r}
x <- runif(20) # 20 randum numbers from uniform distribution [0,1]
ifelse(x >= 0.6, "G", 
       ifelse(x<=0.4, "B", "N"))
```

### Looping

R distinguishes two types of loops:

- ones that execute a function a predetermined number of times, as determined by an *index* [`i`]
- ones that execute a function until a condition is met

The `for()` loop construct takes the following form:

```{r, eval=FALSE}
for (i in seq) expr
```

Again, using curly braces is usually preferred, for loops can be nested and the indices need not be integers: 

```{r}
mat <- matrix(NA, nrow=3, ncol=3)
for (i in 1:3){
  for (j in 1:3){
    mat[i,j] <- paste(i, j, sep="-")
  }
} 
mat
```


While loops take the following form:
```{r, eval=FALSE}
while(cond) expr
```

```{r}
cumsum <- 0
while(cumsum <= 3) {
  cumsum <- cumsum + runif(1)
  print(cumsum)
}
```
A repeat loop is similar, but we must explicitly add a `break` to specify when to exit the loop:
```{r}
cumsum <- 0
repeat {
  cumsum <- cumsum + runif(1)
  print(cumsum)
  if (cumsum >= 3) break
}
```

Both of these constructs should be used with great care, as careless specification of the exiting condition can leave you stuck in an infinite loop. Try running the last example without the line specifying the break! Luckily RStudio allows you to interrupt such an endless loop using the little red stop button in the top right corner of the console window. 

## Vecotrisation  and `apply` family of funcitons

Looping functions - the `for()` loop in particular - are very intuitive and mastering them can represent a quick capability boost for a new R programmer. It is however highly recommended that you spend some time mastering the related `apply` family of functions, which should cover most of your looping needs. The general rule is this: 

> *If you need to apply an expression over a series of elements and the* order *in which you do this is important, then use a loop. If the order is not important, take advantage of `apply`. In many circumstances this can improve the speed of your code, but in all cases it will make your code simpler and easier to read.*

The underlying logic of the `apply` family is this: starting out with some data structure (a vector, matrix, data.frame etc.), we want to split it into constituent parts, apply a function on each of them, and combine them back^[This idea comes from Hadley Wickham's paper on the split-apply-combine strategy of data analysis:  [http://vita.had.co.nz/papers/plyr.html](http://vita.had.co.nz/papers/plyr.html)]. We might for example want to apply a function on every row of a data.frame, every element of a vector, or every column in a matrix. 

### `apply()` 

The `apply()` function will apply a function to either the rows or the columns of a matrix. It's basic structure is: 

```{r, eval=FALSE}
apply(X, MARGIN, FUN, ...)
```
Where X is a matrix (if it is a data.frame, R will coerce it to a matrix), `MARGIN == 1` indicates rows, and `MARGIN == 2` indicates columns. A simple example of its use is to calculate row and column totals:

```{r}
mat <- matrix(1:9, 3,3)
# row totals
apply(mat, 1, sum)
# column totals
apply(mat, 2, sum)
```

In passing the function `FUN` in the example here we used built in function `sum`, but the real power of `apply` comes from integrating it with user defined functions. These are covered in the next section, but here is a quick example of how an in-line function can be used to find the second largest value in each row of a matrix. 



```{r}
mat <- matrix(sample(1:100, 25), 5,5)
mat
# find the second largest value in each row
apply(mat, 1, function(x) sort(x, decreasing = TRUE)[2])

# and for comparison, here is how we would do this using a for loop
out <- vector()
for (i in 1:nrow(mat)) {
  out[i] <- sort(mat[i,], decreasing = TRUE)[2]
}
out
```


### `lapply()` and `sapply()`

The functions `lapply()` and `sapply()` both apply a function to a vector, and the first returns a list back, while the second will try to simplify and return a vector. 


It is important to note that in R there are two types of vectors: i) atomic vectors and ii) lists. 

Furthermore, data frames in R are also represented as lists, with each column is an element of the list, represented by a vector. 

So this means both these functions can be applied to atomic vectors, to data frames, or to other types of lists:

```{r}
# a list of elements with different lengths:
test <- list(a = 1:5, b = 20:100, c = 17234) 
lapply(test, min)
sapply(test, min)

# a data frame (list of three vectors of equal length):
test <- data.frame(a = 1:5, b = 6:10, c = 11:15) 
lapply(test, mean)
sapply(test, mean)

# an atomic vector (this is rather silly, since sqrt(X) would work the same)
# but is added for completeness
test <- 1:3
lapply(test, sqrt)
sapply(test, sqrt)
```

By writing more elaborate functions and passing them as the argument to any of the `apply` family of functions, this seemingly simple construct can become incredibly powerful - as well as making the code eminently readable.


## Writing your own functions

One of the greatest strengths of R comes from writing your own functions. This not only allows you to repeat the same procedure consistently, but makes your code more structured and readable reduces chance of error, and will further strengthens your reproducibility mentality. 

The basic construct is as follows:
```{r, eval = FALSE}
function.name <- function(arguments, ...) {
  expression
  (return value)
}
```

we have already seen the in-line version of the function call in the apply example above, remember:
```{r, eval = FALSE}
function(x) sort(x, decreasing = TRUE)[2]
```

Here our function takes a single argument (`x`), evaluates the expression (`sort(x, decreasing = TRUE)[2]`), and returns the value of that expression. This only works if the function has only a single expression, in which case the evaluated expression is returned. Otherwise we have to explicitly state what we want returned. We can rewrite this function in the more elaborate mode:


```{r}
FunSecondLargest <-function(x) {
  r <- sort(x, decreasing = TRUE)[2]
  return(r)
}
# now let's try it out on the population data:

FunSecondLargest(tidy.population2010$population)  
```

We can also now use this function directly in the apply call we used before:

```{r}
apply(mat, 1, FunSecondLargest)
```

Now let us rewrite the function so that instead of the second largest, it will find the *n-th* largest value in the vector: by adding an additional argument `n`. And don't forget to write sensible commentary about what you are doing - at least for the benefit of your future self!

```{r}
# Function for extracting the n-th largest value from a vector
# Agruments:
#   x - vector 
#   n - optional integer value for rank, default = 1
# Output:
#   Returns single value 

FunNthLargest <-function(x, n=1) {
  r <- sort(x, decreasing = TRUE)[n]
  return(r)
}

# by default n=1, so it will find the largest value if we don't specify
FunNthLargest(tidy.population2010$population)  
FunNthLargest(tidy.population2010$population, n=2)  
FunNthLargest(tidy.population2010$population, n=3)  
```

We could e.g. further generalise this function to look for the n-th smallest value, by adding another argument for the `TRUE/FALSE` value that gets passed to `decreasing` etc. Note that unlike the argument `x`, the argument `n` has a default value (=1). This means we do not have to explicitly specify it unless we want it to be a different value. 

Functions have their own local environment, which is not accessible from the global environment. This means that whatever calculations are evaluated inside the function call do not clutter your workspace, but also their results are not accessible unless you explicitly return them from the function. Thus the object `r` will not be found in the global environment, instead we will get the error:
```{r, eval = FALSE}
r
Error: object 'r' not found
```

We can also have our function return several outputs for example:

```{r}
FunNthLargestElaborate <-function(x, n=1) {
  r <- sort(x, decreasing = TRUE)[n]
  description <- paste("Rank", n, sep=":")
  return(c(description, r))
}
FunNthLargestElaborate(tidy.population2010$population, 3)
```

As we have seen before with if/else statements and loops, functions can also be nested -- as well as combined with if/else statements and loops! It is good practice to try to keep your code modular: keep your functions short and call them from each other. This again makes it easier for the reader to understand what is going on, and easier for you to find errors or update your code. 

From a project management point of view it is also good practice to store all your functions in a separate file, which you `source()` at the beginning of each session. You can even add `source("00-MyFunctions.R")` to your `.RProfile` file, which means all your bespoke functions will be automatically uploaded at the start of each session. 

\newpage
# PRACTICAL 2: If/else, loops, apply and functions {-}

In this second practical we will: 

(i) Practice conditional expressions and logical operators 
(ii) Write a loop with an if-else expression
(iii) Write a function to vectorise the same idea 
(iv) Write a function to draw a plot

## P2.i: If/else, loops, apply and functions {-}
```{r}
## 3.1 Practice conditional expressions and logical operators 
###############################################################################
```
Practice conditional expressions and logical operators by seeing if you can figure out the results of the following expressions in your head, then check them in R:
```{r}
x <- 1:7
y <- -3:3
x
y
```
```{r, eval=FALSE}
# try the following: 
(x == y)
(x > abs(y))
(x > 3) & (x < 5)
(x > 3) | (x < 5)
xor((x > 3), (x < 5))
(-1 %in% y)
(3 %in% y) & (3 %in% x) 
(3 %in% y) & !(3 %in% x) 
```


## P2.ii: Write a loop with an if-else expression {-}


```{r, eval = FALSE}
## 3.2 Check consistency of tidy.economic.situation proportions using IF - ELSE
###############################################################################
```
Use a `for()` loop to go through every row of `tidy.economic.situation` (tip: `nrow()` will tell you how many iterations you need):

* for each row add up the proportions for all three answers (columns two to four)
* use an `if/else` construct to check if the total equals 100
    + if it does, use `print` to print out an OK message
    + if it doesn't, print out a different message, one created using `paste` - so you can include the information on *which* row you have found the error.

Use the following template to write your loop:
```{r, eval = FALSE}
for (i in 1: nrow(tidy.economic.situation)) {
  if(???){
    ???} else {
      ???}
}
```

You should get the following output: 
```{r, echo = FALSE}
for (i in 1: nrow(tidy.economic.situation)) {
  if(sum(tidy.economic.situation[i,2:4]) == 100){
    print("OK")} else {
      print(paste("Something is wrong in row ", i))
    }
}

```

## P2.iii Write a function to vectorise the same idea {-}

```{r, eval = FALSE}
## 3.3 Write a function to check consistency of tidy.economic.situation proportions
###############################################################################
```
Now write a function for the row checking you just did inside the `for()` loop. This is simply generalising the if/else expression to take a supplied argument instead of explicitly naming the row:

* Make sure you *document* your function correctly!
* the input for the function should be a row
* the output of the function should be  a variable called `message` - "OK" or "Not OK"
* Use the framework below:

```{r, eval = FALSE}
FunRowCheck <- function(x) {
  message <- if(???){
    ???} else {
      ???}
  return(message)
}
```

Now test out your function on a single row:
```{r, echo=8:10}
FunRowCheck <- function(x) {
  message <- if(sum(x) == 100){
    "OK"} else {
      "Not OK"}
  return(message)
}

## 3.4 Use your functions inside an apply statement
###############################################################################

# append the new test variable to the dataset
tidy.economic.situation$test <- apply(tidy.economic.situation[,2:4], 1, FunRowCheck)

FunRowCheck(tidy.economic.situation[1,2:4])
```

If it works correctly, you can now try using your new function inside an `apply` construct. 

Remember, `apply` will evaluate the expression along the *whole* row, and you want to apply it only to columns 2:4, so make sure you don't pass the whole table to `apply`. When you are happy with the result, append it to the table as an additional column:

```{r, eval = FALSE}
tidy.economic.situation$test <- apply(???)
```

Your table should now look like this:
```{r}
tidy.economic.situation
```

If it doesn't have a look at the `scripts/02-FunctionsAndLoops.R` file in the scripts folder for the solution before you proceed to the next task. 


## P2.iv Write a fucntion to draw a plot {-}
```{r}
## 3.5 Bar-Plotting function
###############################################################################
```
To finish off this practical session we will write a function for plotting our table, using the `barplot()` function. This function only accepts vectors or matrices, but because our data is in a data.frame, we need to use `as.matrix()` for it to work, in addition to `t()` for transposing it. Here is the code for the most stripped down stacked barplot of the people's perceptions of their household financial situation. Note that the order of the columns had to be changed to make the more logical order of bad - neutral - good. 
```{r}
barplot(t(as.matrix(tidy.economic.situation[1:6,c(2,4,3)])))

```

Now expand the barplot function - use the help documentation in the help tab:

- add names to the x-axis
- add legend text
- add a main title
- feel free to explore additional arguments to the plot!

Once you are happy with your plot, enclose it in a function, so that you can pass it each of the three subsets of the table individually, and the function will additionally also change the plot's title to the correct one. 

You should now be able to run the following lines to produce all three charts. If you are having issues, check the code in `scripts/02-FunctionsAndLoops.R` for the solutions. 
```{r, eval = FALSE}
FunBarplot()
FunBarPlot(perception = "UK")
FunBarPlot("W")
```
\newpage
# Data manipulation with `dplyr`

Finally, the relatively new `dplyr` family of functions is one of the most powerful recent developments in the R coding world. While all of the data processing capabilities of `dplyr` existed in one shape or another in R before (usually several), `dplyr` brings them together in a comprehensive and systematic way, that allows for cleaner code that is easier to read, and faster to run. It also integrates logically with the `tidyr` family of functions described earlier, but it's most exciting and revolutionary aspect is it's assimilation of the *piping* or *chaining* of successive data processing functions - originally developed in the `magrittr` package, and now rightly becoming mainstream R practice. 

We will cover some of the most important functionalities of `dplyr` here, but you are encouraged to explore several excellent on-line resources, including video tutorials etc. The [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) is only a click away under `Help/Cheatsheets`!

For this whole section we will be using the two data tables  prepared in the first practical, so make sure you have both `tidy.population2010` and `tidy.economic.situation` in your environment. 

## Subsetting

### `filter()`
Extracts rows that meet the logical criteria:
```{r, eval = FALSE}
filter(data, criteria)
```

You can use any of the evaluation conditions and logical operators  we covered at the beginning of the previous section:

```{r}
filter(tidy.population2010, AREA_KM2 < 1000 & population > 10000 & AGE == 0)
```

### `select()`
Extracts only the columns that you list:
```{r, eval = FALSE}
select(data, list)
```

```{r}
select(tidy.economic.situation, bad, good, neutral)
```

In addition there is a large number of helper functions to select column names:

```{r}
# all the columns between bad and neutral
head(select(tidy.economic.situation, bad:neutral), n=3)
# all but the perception column
head(select(tidy.economic.situation, -perception), n=3)
# contains a dot in the name:
head(select(tidy.economic.situation, contains(".")), n=3)
# starts with the letter p
head(select(tidy.economic.situation, starts_with("p")), n=3)
# ends with the letter d
head(select(tidy.economic.situation, ends_with("d")), n=3)
# contains the text "n"
head(select(tidy.economic.situation, contains("n")), n=3)

```

You can also use select to reorder the columns, in our case we might want to reorder the three answer columns so:
```{r}
# change order of columns
head(select(tidy.economic.situation,income.group:bad, neutral, good:perception), n=3)
# we can also do this using the columns' respective indices instead
head(select(tidy.economic.situation, 1,2,4,3,5), n=3)
```

## Making new variables

New variables are easily created using `mutate()`, which has the additional advantage of allowing you to reuse variables as you create them, without the need for an intermediate step!
```{r}
# scale the values so they all sum up to 100
tidy.economic.situation <- mutate(tidy.economic.situation, total = bad+neutral+good, 
                                  bad.scaled = bad/total*100,
                                  good.scaled = good/total*100,
                                  neutral.scaled = neutral/total*100,
                                  total.scaled =  bad.scaled  + good.scaled + 
                                    neutral.scaled
)
head(tidy.economic.situation)
# we can now use select to remove the old ones
tidy.economic.situation <- select(tidy.economic.situation, -bad, - good, -neutral, 
                                  -total, -total.scaled)
# we could also use rename to rename the new ones
tidy.economic.situation <- rename(tidy.economic.situation, bad = bad.scaled, 
                                  good = good.scaled, neutral = neutral.scaled)
```


New variables can also be made using existing or user written functions. For example using `cut()` we can recode the bad variable into a categorical one:
```{r}
head(mutate(tidy.economic.situation, bad.cat = cut(bad, seq(0,100,10))))
```


## Summarizing
We can quickly summarise the data column wise using the `summarise()` function
```{r, eval = FALSE}
summarise(data, new.var = summary.function(column))
```

For example the average population, average area and total count in the population table, we can also use our own functions, such as the one we wrote before to get the second largest population value
```{r}
summarise(tidy.population2010, pop = mean(population), area = mean(AREA_KM2), count = n(), 
          test = FunSecondLargest(population))
```

But the summarise function really comes into it's own when it operates on a *grouped* table. Using the function `group_by()` the table is (invisibly) split into sub-tables by the values of the grouping variable, and the summarise function then operates on each subset individually:
```{r}
summarise(group_by(tidy.population2010, AGE), 
          av.pop = mean(population), av.area = mean(AREA_KM2), count = n(), 
          test = FunSecondLargest(population))
```

An important corollary to the grouping function is `ungroup()`, which removes the grouping from the table for further analysis -- we will use it in the last section of this chapter.

## Joining tables
The `dplyr` package also contains a set of functions that allow you to join tables by matching on common variables namely:

- left_join(a, b) 
- right_join(a, b) -- keeps all 
- inner_join(a, b) -- only keeps rows present in both a and b
- full_join(a, b) -- keeps all rows


```{r}
# prepare two small tables, one of UK men aged 0 or 1, the second of women aged 1 or 2:
UK.men <- filter(tidy.population2010, FIPS == "UK" & sex == "male" & (AGE == 0 | AGE == 1 ))
UK.women <- filter(tidy.population2010, FIPS == "UK", sex == "female" & (AGE == 1 | AGE == 2 ))

# try out all 4 merges on the two tables
left_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
right_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
inner_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
full_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
```

All of these have a `by=` argument, which lets you choose the columns to be joined by - if you do not explicitly name them, `dplyr` uses all the ones with identical names in both tables. If the columns you want to join by have different tables in each table you can specify this so: `by = c("name.a" = "name.b")`

### Other `dplyr` funcitons
The [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) is an indispensable help with `dplyr` functions. We will briefly mention only one more, but there are several more that we will not cover here.

`arrange()` for data sorting - ascending by default, otherwise specify `desc()`:
```{r}
arrange(tidy.economic.situation, perception, desc(bad))
```


\newpage
## Piping/chaining daisies 

*Piping* data brings a completely new level of intuitiveness you R programming. Instead of nesting and indenting successive functions, which means the code has to be read *inside out*, piping (also known as daisy chaining) allows the code to be written in the natural direction in which the data is flowing.  The piping operator `%>%` indicates the direction of this flow as well, taking the output of the preceding function and directing it into the next one. 

Piping can be applied to almost any function, but shines particularly brightly when combining the dplyr functions we have just covered. For a pretty silly example:

```{r}
tidy.population2010 %>%
  filter(AREA_KM2 < 2000 & population > 15000 & AGE == 0) %>%
  select(-AGE) %>%
  mutate(density = population/AREA_KM2) %>%
  group_by(NAME) %>%
  summarise(count = n(), mean.density = mean(density)) 
```

You will note that we skipped naming the data object in all of functions. The piping operator means it is implicit what the data being passed on is, so there is no more need to explicitly name it. 

In keeping with the piping logic, we can also use a rarely used R assignment operator: `->`. We can add it at the end of the pipe/chain and point it to the new object's name. Of course you can also start the way we have been starting all along, and assign in the standard direction `<-` if you prefer. 

A whole set of piped functions can easily be wrapped up in a function: 
```{r}
FunMyPipe <- function(x) {
  x %>% 
    sqrt %>%   # square root
    mean %>%   # mean  
    "*" (100)  # multiplication - a bit awkward, true
}
# Test it out on a short vector
FunMyPipe(1:10)
```

\newpage

# PRACTICAL 3 - Piping Population Pyramids {-}

In this final practical we will: 

(i) Practice piping with `dplyr` functions
(ii) Combine piping with function writing 
(iii) Use that to create a bespoke population pyramid plot function

## P3.i: Practice piping with `dplyr` functions {-}

```{r}
## 4.0 Practice piping on population2010
##########################################################
```
Using the `tidy.population2010` table find the answers to the following:

* How many 20 year-old males were there in Tanzania in 2010
* Which country has the lowest total population?
* In which country do women outnumber men in the most age groups?

## P3.ii: Combine piping with function writing  {-}

```{r}
## 4.1 Write a function to extract population pyramid data
###############################################################################
```
Write a function that uses piping and `dplyr` functions to do the following:

* the input is the FIPS country code (if you're not sure which code means which country, check the list here: [https://en.wikipedia.org/wiki/List_of_FIPS_country_codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes))
* from  `tidy.population2010` extract the data for that country, and
* remove variables for the area, and
* create a new variable grouping the ages into 5 year age groups (use `cut(x, 20)`), and
* find the sum of the population for each age group and gender combination (use `group_by`!), and
* don't forget to `ungroup` the data before the next step!, and
* create a new variable representing the proportion of the total population in each age/sex combination (* 100), and
* return this table, which should have 40 rows and 4 columns.


## P3.iii: Combine piping with function writing with plotting {-}

```{r}
## 4.2 Write a function to draw a population pyramid plot
###############################################################################
```
We will use the `plotrix` package for this, although you are free to experiment with drawing your own pyramid plot - you will have much more control than using the default `pyramid.plot()` function. Have a look at the help documentation for this function. In particular note that you need to provide it two vectors, `lx` and `rx` for the population sizes. 

Write a function that:

* takes as it's input the output from your previous function (the 40x4 table)
* creates the `lx` and `rx` vectors
* **IMPORTANT** again you will have a data.frame as the result. use `as.matrix` on `lx` and `rx` so they can be used in the plot
* calls pyramid.plot(lx, rx) as well as any other arguments you may want to add. (in particular you may want to add `labels=`). One way of creating them is `paste(seq(0,96, 5), seq(5,100,5), sep="-")`, but you can also use the values from the age group variable. 




```{r, echo = FALSE}
FunPopClean <- function(cntry = "UK"){
   tidy.population2010 %>%
    filter(FIPS == cntry) %>%
    mutate(age.g = cut(AGE, 20)) %>%
    group_by(age.g, sex) %>%
    summarise(population = sum(population)) %>%
    ungroup() %>%
    mutate(prop = 100*population/sum(population)) ->
    data
  return(data)
}

FunPlot <- function(data, name = NULL){
  data %>%  
    filter( sex=="male") %>%
    select(prop) %>% 
    as.matrix -> lx
  data %>%  
    filter( sex=="male") %>%
    select(prop) %>% 
    as.matrix -> rx
  pyramid.plot(lx, rx,
               main=name,
               labels = paste(seq(0,96, 5), seq(5,100,5), sep="-"))
}
```

```{r fig.width = 6, fig.height = 5 }
FunPlot(FunPopClean("BG"), "Bangladesh")  
  
```

The solutions for this final set of exercises can be found in `scripts/03-PipintAndPyramids.R`. 
