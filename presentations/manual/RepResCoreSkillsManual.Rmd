---
title: "R: Core Skills for Reproducible Research"
subtitle: "Course Manual with Practical Exercises"
author: "Maja Zalo&#x17e;nik"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true

---
```{r setup, echo=FALSE, message=FALSE, warning = FALSE}
require(knitr)
require(dplyr)
require(tidyr)
require(plotrix)
opts_knit$set(root.dir = "C:/Users/sfos0247/Dropbox/XtraWork/R stuff/RepResCoreSkillsR")
opts_chunk$set(warning=FALSE, message=FALSE)
options(scipen=999, digits=4)
```
\newpage
# Introduction
## Blurb

This short course covers the core skills required for a budding R user to develop a strong foundation for data analysis in the RStudio environment. Within the framework of a reproducible research workflow we will cover importing and cleaning data, efficient coding practices, writing your own functions and using the powerful `dplyr` data manipulation tools. 

## Key Topics

* Reproducible Research
* R Studio and project management
* Importing and cleaning data
* Good coding practices in R
* standard control structures
* Vectorisation and `apply` functions
* Writing your own functions
* Data manipulation with `dplyr`
* Piping/chaining commands


## Course information

**Intended audience**	Anyone interested in quantitative data analysis using open source tools.

**Prior knowledge** Knowledge of R (as covered in R: An introduction).

**Resources**	Course handbook

**Software** RStudio & 	R 3.1.2

**Format**	Presentation with practical exercises

**Where next?**	R:

\newpage

# Reproducible Research 

Reproducible research means making the data and the code of our analysis available in a way that is sufficient and easy for an independent researcher to recreate our findings. 

This is the golden standard of scientific inquiry, and is increasingly and rightly becoming a requirement in academic publishing, and by funding bodies. 

It is also a way of establishing better working habits, reduce the potential for error, develop a more streamlined research process, and make for easier collaboration. 

Reproducible research does take a bit of upfront investment in learning the tools and setting up your workflow. Luckily RStudio has integrated many of the tools required in one platform, making it easier than ever to 

## Why?

* Reinhart Rogoff Excel spreadsheet

Document everything! This means never running any code from the command prompt, always writing it into a script file and running it from there. 


#  Set-up 
## RStudio
## Project management

A crucial requirement for conducting reproducible research, and one that has to be carefully considered before you embark on your analysis, is your plan on how the data, code and outputs will be organised. The project management structure proposed here is just a suggestion, and you should adapt it to your specific needs, but it is highly recommended that you stick to one such system consistently, instead of coming up with 'ad hoc ' solutions for every new project. 

RStudio makes it extremely easy to divide your work into separate projects, allowing you to neatly organize and access your work. 



## Literate programming

### Consistent coding style e.g.:
![Code Quality part 2. (https://xkcd.com/1695/)](../../figures/code_quality_2.png)

* [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml)
* [Hadley Wickham's Style Guide](http://adv-r.had.co.nz/Style.html)

### File formats
Human readability of data files and outputs. Future-proof. 
.txt files

### Commenting 






# Workflow
> *Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.* 
<p style="text-align: right;"><a href="http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0">source: NY Times</a></p>

## Importing data

Regardless of whether your data is stored locally or downloaded from the web, you should never manipulate the original data directly. This is crucial for the integrity of your reproducible research process. 

R has utilities for importing data from a wide variety of sources including proprietary formats. Ideally you want to be working with .csv files, as they are the cleanest and least problematic to import, but often you have no choice in the matter. In the practical we will import .csv, .xls and .sav files, including downloading and unzipping them. Here is a list of some common formats and the packages used for importing them, refer to the help pages for more details: 

* comma separated values -- `read.csv()` 
* tab-delimited text file -- `read.table()`
* other delimited files -- `read.delim()`
* Minitab -- `read.mtb()` from  `library(foreign)`
* SPSS -- `read.spss()` from `library(foreign)`
* Stata -- `read.dta()` from  `library(foreign)`
* Excel -- `read.xls()` from `require(gdata)`
* Excel -- `loadWorkbook()` from `library(XLConnect)`

The basic import functions of the `read.table()` family all have a `nrows` argument, which is particularly useful if you do not know the structure of the data and are dealing with a large fine. In which case it is recommended you try a test import with e.g. `nrows=10`, and check the result before attempting to import the full file. 

For a more comprehensive list of possible input formats see this tutorial: [https://www.datacamp.com/community/tutorials/r-data-import-tutorial](https://www.datacamp.com/community/tutorials/r-data-import-tutorial)



We will store all our data files in the `data` folder of our project, from where they will be imported into R. This means the original files remain *untouched* by the data analysis and should never be overwritten as the result of your analysis. 

While you might find it easier to simply download a file into your folder, this poses the problem of loosing track of where the data was sourced from. It is therefore highly recommended you download the data programmatically if possible, and if not, that you use comments within the code to describe the source of the files. For example the `pop2010.csv` file we downloaded in the first practical should have been downloaded directly from within R, by doing it manually we are reducing the reproducibility of our project. We must therefore make sure we note the origin and date we accessed the data in our code!


## Data tidying

> *Tidy datasets are all alike but every messy dataset is messy in its own way.* – Hadley Wickham

A great deal of data tidying can be done manually with the base R functions. Additionally there are several packages available with more specific functions. In this course we will use the `tidyr` package by Hadley Wickham, which is particularly well integrated with the `dplyr` package we will be using in the second part of this course. 

The underlying principle of the `tidyr` package is *tidy data*, which must satisfy the following three principles:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Source: H. Wickham (2014) *Tidy Data* (available:  [http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf))

This may seem trivial, but it is in fact common to encounter data that does not conform to these principles. The four workhorse functions of `tidyr` that should solve all your data tidying needs are:

* `spread()`
* `gather()`
* `separate()` 
* `unite()`


### spread()

Below we can see an example of a messy table, since each observation is in fact represented in two rows. Third column in fact contains variable names (`density` and `population`), while the fourth column contains their values. 

```{r, echo=FALSE}
messy.02 <- read.table("data/demo/messy.02.txt")
messy.02
```

we can using `spread()` we can tidy this layout:  

```{r, echo=c(3:4)}
require(tidyr)
require(dplyr)
tidy.02 <- spread(messy.02, key, value)
tidy.02
```

The syntax for `spread()` takes the following form:

`spread(data, key, value)`

The *key--value* pair is the underlying logic of the tidy data table. We can decompose the data into a collection of key--value pairs such as this:

Key : Value
```{r, eval=FALSE}
Country: Norway
Country: Slovenia 
Country: UK

Year: 2010
Year: 2050

Population: 4891300
Population: 2003136
...

Density: 16.07489
Density: 20.91484
...

```

In a tidy data table each cell contains a *value* and the *keys* are the column names. 

### gather()

Here is another messy table:
```{r, echo=2}
messy.01 <- read.table("data/demo/messy.01.txt")
messy.01
```

Now we have three variables: the country, which is in the first column, the year, which is across the header row (representing the *keys*), and the population (representing the *values*), which is in the second and third columns. Using `gather()` we can tidy up the table, so that now each of the three variables has its own column, and each row is an observation:

The syntax for `gather()` takes the following form:

`gather(data, key, value, ...)`

where the `...` represents the columns we want to gather, in our case columns 2 and 3. The key and value arguments are the *names* of the two new variables, or columns we are creating: the *key* is currently in the column names of columns two and three - so we want it to become `year`, and the *values* are in the cells of those two columns, so we want it to become `population`.  


```{r, echo=c(3,5)}
require(tidyr)
require(dplyr)
tidy.01 <- gather(messy.01, year, population, 2:3)
tidy.01 <-  mutate(tidy.01, year=ifelse(year=="X2010", 2010, 2050))
tidy.01
```


### separate() and unite() 

Separate and unite are straightforward helper functions for the reshaping done by gather and spread. The following table for example requires spreading, but the `double.key` variable contains both *values* (years) and  *keys* (population and density):

```{r, echo=FALSE}
messy.03 <- read.table("data/demo/messy.03.txt")
messy.03
```

These are separated simply by the following code into `year` and `key`, which can then be used to reshape the table as we did above. 

```{r, echo=FALSE}
tidy.03 <- separate(messy.03, double.key, c("year", "key"))
tidy.03
```

The function `unite` is the inverse of `separate`, and merges the values of selected columns into a new single column. In both cases you can change the separator using the `sep=` argument. 
```{r, echo=FALSE}
messy.again <- unite(tidy.03, new.double.key, key, year,  sep = " in the year ")
messy.again
```

For an excellent write-up of the main `tidyr` functions see Garrett Grolemund's post here [http://garrettgman.github.io/tidying/](http://garrettgman.github.io/tidying/).

For a quick tidyr cheat-sheet stick this to your wall: [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

## PRACTICAL: new R project


The complete documentation for this course is available as an RStudio project in a public github repository: [https://github.com/majazaloznik/RepResCoreSkillsR](https://github.com/majazaloznik/RepResCoreSkillsR) or for extra convenience: [http://tinyurl.com/RCSRepRes](http://tinyurl.com/RCSRepRes). 

In this first part of the practical you will set up a new RStudio project mirroring the structure of the github repository for this course. 

Unless you have brought your own laptop, you will be completing this project on the local drive of the computers here. This unfortunately means you will have to transfer your work via USB, email or other means in order to keep it for your record. The complete materials will however remain available to you on-line at the above addresses. 

### Create new RStudio project

1. Open RStudio
2. Select the project menu in the top right-hand corner and select `New Project`
3. Select New Directory and choose the name of your project (e.g. RRepResCourse)^[As a general rule you should avoid spaces in file and folder names, although you will probably be fine if you ignore this advice.].  
4. RStudio has now created a new project file in your folder, which you can see in the Files pane.
5. Run the command `getwd()` in the console. RStudio automatically sets the working directory in the top level of your project folder. 
6. Click on the project menu again and select Project Options. On the options "Restore .RData to the workspace at start-up" and "Save workspace to .RData on exit" select No. In fact, you should set that as a global option (Tools/Global Options) -- for truly reproducible research you should never have to load a previous workspace!

### Folder and File Structure

1. Using the New Folder button in the Files pane, create three folders called (some equivalent of)
  * data
  * scripts
  * figures
  * presentations
2. For the rest of this practical, all you need is to download the file `pop2010.csv` from the github repo into your `data` folder. 
3. You can also download the manual and slides to your `presentation` folder. 
4. Create an .RProfile file using the command `file.edit(".Rprofile")`. This will open a new script tab for you to edit. The content of the .RProfile gets automatically run every time you open your project. It is therefore a good idea to e.g. load any packages you will need from your .RProfile, instead of doing it manualy every time. 

For this practical you will need the following packages (type this into your .RProfile file and save)
```{r, eval = FALSE}
require(xlsx)
require(forein)
require(dplyr)
require(tidyr)
require(plotrix)
```


## PRACTICAL: Import and clean some data

### Downloading and importing data

First create a new `.R` script file in your `scripts` folder or equivalent via the drop-down menu or using `Ctrl+Shift+N`. Naming  it something like `01-DataImport.R` will make your project management easier in the long run, but feel free to set up your own file naming system -- but try to stick with it!

It is good practice to establish a header system for all your script files, such as the one below. The `#` lines are also a good way of making the code file structure easy to understand. Following the [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml) rule "The maximum line length is 80 characters.", a nice little trick is to make these separators 80 hashtags long, which gives you a nice visual reference for when your code gets too wide. 

```{r}
###############################################################################
## DATA IMPORT AND CLEANUP
###############################################################################
## 1.1  Import a .csv file
## 1.2  Download and import an Excel file
## 1.3  Download, unzip and import a .dat file 
###############################################################################

```

Make sure your working directory is at the top of your project folder using `getwd()`. We are going to use the `read.csv()` function to import the data from the `pop2010.csv` file in the `data` folder. But just to be safe, we will first do a test run, importing only 10 rows, so we can inspect the result before importing the whole table:
```{r }
## 1.1  Import a .csv file
###############################################################################
# don't forget to note the source of the data!
# downloaded manually from https://github.com/majazaloznik/RepResCoreSkillsR/
# blob/master/data/pop2010.csv?raw=true" on 20.6.2016
getwd()

# test run
population2010 <- read.csv("data/pop2010.csv", nrows=10)
population2010
```

That looks good, the only thing is, I can tell you that all the data in this file is from 2010, so we do not really need the last column. In order to skip it during import, we can use the `colClasses` argument, and setting the seventh argument to `NULL`

```{r}
# import full table except for 7th column (year)
population2010 <- read.csv("data/pop2010.csv", 
                           colClasses = c("integer",   # age
                                          "integer",   # area 
                                          "character", # name 
                                          "integer",   # population
                                          "integer",   # sex
                                          "character", # country id (FIPS)
                                          "NULL"))     # year - skip
# check how it looks
head(population2010)
tail(population2010)
```


Comma separated values (`.csv`) is one of the preferred formats to import data from, but R allows you to import from a variety of other formats, although this can sometimes get a bit more messy. This time we will also first download the file, before importing a table from one of the spreadsheets. This is from [https://data.gov.uk/dataset/social_trends](https://data.gov.uk/dataset/social_trends), part of the government's open data access initiative and a great resource!


```{r}
## 1.2  Download and import an Excel file
###############################################################################
# url of the .xls file we want (using paste only to keep code under 80 chars:)
data.url <- paste("http://www.ons.gov.uk/ons/rel/social-trends-rd/social-",
            "trends/social-trends-41/income-and-wealth-data.xls", sep="")

# download location 
data.location <-  paste( "data", "income-and-wealth-data.xls", sep = "/")

# download - Excel files are binary, so set the mode to "wb"!!
download.file(data.url, data.location, mode="wb")
```
You can now have a look in the data folder to check the file has been correctly downloaded and inspect it in Excel. We will import the table from the third worksheet, named "Table 1", on people's perceptions of the current economic situation. Close the Excel file before proceeding! Several solutions are available for importing Excel files into R, a nice overview can be found [http://www.r-bloggers.com/read-excel-files-from-r/](here). In this practical we will use the `xlsx` package:

```{r}
## Importing the data from an .xls file
require(xlsx)
# if you get an error telling you there is no package called ‘xlsx’ run:
# install.packages("xlsx")

# let's see what happens if we import the whole sheet
economic.situation <- read.xlsx(data.location, sheetIndex = 3)
```
Have a look at `economic.situation`. ^[Are you getting an error? That may be because you still have the Excel file open!]  It is not ideal, empty rows and columns are imported, as is the text at the top and the bottom of the worksheet. Luckily, `read.xlsx` has plenty of arguments that allow us to specify more precisely what we want to import. In this case, we can go one step further, and note that there are actually three separate tables in this worksheet, so it might be easiest to import them separately:

```{r}
## using rowIndex and colIndex select each subtable individually:
world.situation <- read.xlsx(data.location, sheetIndex = 3,
                         rowIndex=c(4, 6:8), colIndex = c(1:7))

UK.situation <- read.xlsx(data.location, sheetIndex = 3,
                             rowIndex=c(4, 11:13), colIndex = c(1:7))

household.situation <- read.xlsx(data.location, sheetIndex = 3,
                          rowIndex=c(4, 16:18), colIndex = c(1:7))
```


Finally, sometimes we need to extract the data from a zipped file, this can also be done directly form R ^[This should work even if no winzip utility is installed on the machine?]. And to try out another format we will import an SPSS file as well.^[The data file is supplementary material to the SPSS Survival Manual from a survey designed to explore the factors that impact on respondents' psychological adjustment and well-being. ] 
```{r}
## 1.3  Download, unzip and import a .dat file 
###############################################################################
# url of the .zip file we want
data.zip.url <- paste("http://spss.allenandunwin.com.s3-website-ap-southeast-2.",
                      "amazonaws.com/Files/survey.zip", sep="")
temp <- tempfile()
download.file(data.zip.url, temp)
# check what is in the zip file using list (doens't extract anything)
unzip(temp, list=TRUE)

# Only one file, that's the one we want to extract to the data folder
unzip(temp, "survey.sav", exdir = "data")
unlink(temp)
```

You can now check the `data` folder and you should find the `survery.sav` file there. Even if you don't have SPSS installed on your computer, you can now open it using R and the `foreign` package:

```{r}
require(foreign)
# if you get an error telling you there is no package called ‘xlsx’ run:
# install.packages("foreign")

# import the data as a data frame:
data.location <- paste("data","survey.sav", sep="/")
ed.psy.survey <- read.spss(data.location,  to.data.frame=TRUE)
# check what it looks like
ed.psy.survey[1:5,1:5]
```

Now we have the data, before we continue we'll just do a bit of housekeeping and clear our workspace of the objects we don't need any more (including the survey dataset, which we will not use in this practical)

```{r}
# CLEAN UP!
rm(economic.situation, ed.psy.survey, data.location, data.url, data.zip.url, temp)
```

### Data Tidying

In the second part of this practical we will use the functions from the `tidyr` package to tidy up the two datasets.

```{r, warning=FALSE, message=FALSE}
## 2.1 tidy up the population data
###############################################################################
require(tidyr)
```

The population2010 data.frame is already pretty tidy! The only issue with it is the `SEX` variable, which is coded for men (`SEX==1`), women (`SEX==2`), and both (`SEX==0`). We really only need to remove the rows with the values for (SEX==0), but we can use this opportunity to perform a data check as well, while practising the `spread()` and `gather()` functions:

First let's try out our technique on a small subset of the data - this is good practice in general, especially if you are dealing with large datasets. We'll select only the observations for Aruba, and have a look at them:
```{r}
# try out our technique on a smaller subset of the data
test.data <- population2010[population2010$FIPS == "AA", ]
head(test.data)
```

We want to reshape the table so that the values of `SEX` will become new column names (i.e. *keys*), and that the *values* for these new keys will be the values from the variable `POP`. This means the `spread()` functions should look like this:

```{r}
tidy.test <- spread(test.data, SEX, POP )
head(tidy.test)
# and for clarity, let's rename the columns:
colnames(tidy.test)[5:7] <- c("both", "male", "female")

```

We can now check if the totals for men and women actually match, before we discard the column with the sum of both: 
```{r}
# calculate sum of males and females
tidy.test$check <- tidy.test$male + tidy.test$female

# compare it with the values already in the table:
all.equal(tidy.test$both, tidy.test$check)

# looks good, now we can remove both total columns:
tidy.test$check <- NULL
tidy.test$both <- NULL

# so now we have:
head(tidy.test)
```

And finally we have to use `gather()` to get back to a tidy table. Remember, with gather you need to pass the *names* of the new variables that are now the *key* and the *value*, and the column names which hold them:
```{r}
tidy.test <- gather(tidy.test, sex, population, 5:6)
# and let's check it again:
head(tidy.test)
```
If you are happy with the test run, you can now try it on the whole `population2010` table. 
```{r, echo=FALSE}

require(tidyr)
# SEX is the key:
tidy.population2010 <- spread(population2010, SEX, POP)

# for clarity, let's rename the columns:
colnames(tidy.population2010)[5:7] <- c("both", "male", "female")
head(tidy.population2010)

# Now check if the sums are right, by creating a new column:
tidy.population2010$check <- tidy.population2010$male + tidy.population2010$female
all.equal(tidy.population2010$both, tidy.population2010$check)

# looks good, now we can remove both total columns:
tidy.population2010$check <- NULL
tidy.population2010$both <- NULL

# now tidy it up again:
tidy.population2010 <- gather(tidy.population2010, sex, population, 5:6)
```
```{r}
## 2.2 tidy up the perception data
###############################################################################
```
Most of the time you will not be lucky enough to work with as nicely formed datasets as the population one. But the same tools can be used to disentangle much more messy tables, such as the ones we extracted from the Excel file above. 

Let's have a look at one of the three files, e.g. `household.situation`, and see how it could be tidied up. What are the variables (that should be in the columns), and what are the observations (that should have one row each)? 
```{r}
# first let's remane the column names
colnames(household.situation) <- c("perception", "inc.lt.20", 
                                   "inc.20.to.39", "inc.40.to.59", 
                                   "inc.60.to.99", "inc.gt.100", "all")
household.situation
```
 
In fact the whole table needs to be transposed, so that each population group represents one observation, and the proportion answering each question are the variables. In order to do that we need to first gather the data in long form, before spreading it out again wide.^[The `t()` function will transpose a data frame in R, try it out to see if it is a useful alternative to gather and spread. ] 

```{r}
# transpose using gather and spread  
X.household.situation <- gather(household.situation, income.group, proportion, 2:7)
tidy.household.situation <- spread(X.household.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.household.situation) <- c("income.group", "bad", "good", "neutral")
# check the result and remove the temporary table
tidy.household.situation
rm(X.household.situation)
```

Don't forget, we have two more tables, one for each perception question. If we want to merge them together at the end, we need to be clear that each observation refers to one of the questions:

```{r}
tidy.household.situation$perception <- "HH"
```

Now you can repeat the tidying for the UK and world perceptions, and once all three tables are tidy, you can merge them together using `rbind()`:

```{r, echo=c(18:19)}
## 2.2.2. UK SITUATION

# first let's remane the column names
colnames(UK.situation) <- c("perception", "inc.lt.20", 
                                   "inc.20.to.39", "inc.40.to.59", 
                                   "inc.60.to.99", "inc.gt.100", "all")
# transpose using gather and spread  
X.UK.situation <- gather(UK.situation, income.group, proportion, 2:7)
tidy.UK.situation <- spread(X.UK.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.UK.situation) <- c("income.group", "bad", "good", "neutral")

# add variable for the question asked
tidy.UK.situation$perception <- "UK"

## 2.2.3. WORLD SITUATION
# first let's remane the column names
colnames(world.situation) <- c("perception", "inc.lt.20", 
                            "inc.20.to.39", "inc.40.to.59", 
                            "inc.60.to.99", "inc.gt.100", "all")
# transpose using gather and spread  
X.world.situation <- gather(world.situation, income.group, proportion, 2:7)
tidy.world.situation <- spread(X.world.situation, perception, proportion)
# let's also rename the column names in keeping with the convention of avoiding spaces
colnames(tidy.world.situation) <- c("income.group", "bad", "good", "neutral")

# add variable for the question asked
tidy.world.situation$perception <- "W"

# merge all the tables together
tidy.economic.situation <- rbind(tidy.household.situation,
                                 tidy.UK.situation,
                                 tidy.world.situation) 
```

Finally, you can now clear your workspace using `rm()` as we did before, to remove everything except for `tidy.population2010` and `tidy.economic.situation`. 


\newpage
# Efficient Coding
This section covers some of the most important skills to improve the efficiency, readability, and reproducibility of your R code. The standard control of the flow of your code that can be achieved with `ifelse` statements and looping is covered briefly, however you are encouraged in particular to explore the advantages of *vectorised* R code. Writing your own functions will greatly streamline your work, as well as forcing you to think in more abstract terms about your analysis - making it easily transferable and reproducible as opposed to limited to the specific situation you find yourself analysing at the moment.




## Standard control structures

An indispensable gain in efficiency of your programming can be achieved by using *control strucures* to control the execution of your code. These can be divided into *conditional execution* structures (`if` and `else` type functions) and *looping* structures. However, as we shall see in the next section, there are some very good reasons to avoid looping in R!

### Conditional execution

The standard syntax for conditional execution is as follows:

```{r, eval=FALSE}
if (condition) {
  # do something
} else {
  # do something else
}
```

In fact, you may also use only the `if()` construct on it's own:

```{r, eval=FALSE}
if (condition) {
  # do something
}
```

The if/else syntax also works in a single line, where you can dispense with the curly braces:

```{r, eval=FALSE}
if (x >= 0)  print("Poz") else print("Neg")
```

While this is more compact, it can impact readability, and can also make your code more difficult to debug and extend. Using curly braces and indenting the code properly will make it clearer to the reader, and also easier to e.g. extend via nesting:

```{r, eval=FALSE}
x <- runif(1) # randum number from uniform distribution [0,1]
if (x >= 0.6) {
  print("Good")
} else {
  if (x <= 0.4) {
    print("Bad")
  } else {
    print("Not Sure")}
}
```

The conditions to be evaluated are: 
```{r, eval=FALSE}
x == y   # x is equal to y
x != y   # x is not equal to y
x > y    # x is greater than y
x < y    # x is less than y
x <= y   # x is less than or equal to y
x >= y   # x is greater than or equal to y
x %in% y # x is located in y
TRUE     # 
FALSE    #
```

And these can further be combined using standard logical operators:

```{r, eval=FALSE}
! x       # NOT
x & y     # AND
x | y     # OR
xor(x, y) # exclusive OR
```

What if you want to run a conditional statement over an entire vector? You might be tempted to jump to the next section on looping, and construct a loop going over each element of the vector and evaluating the condition. This would of course work, but it would be a very inefficient way of coding, and would not be taking advantage of the efficiencies of vectorisation in R (covered in the next subsection). In such cases, you should use the *vectorised* form of the if/else construct:

```{r, eval=FALSE}
ifelse(condition, yes, no)
```

Where `yes` is the value to be returned if the condition is satisfied, and `no` if not. Similarly as above, `ifelse()` statements can also be nested. 
```{r}
x <- runif(20) # 20 randum numbers from uniform distribution [0,1]
ifelse(x >= 0.6, "G", 
       ifelse(x<=0.4, "B", "N"))
```

### Looping

R distinguishes two types of loops:
- ones that execute a function a predetermined number of times, as determined by an *index* [`i`]
- ones that execute a function until a condition is met

The `for()` loop construct takes the following form:

```{r, eval=FALSE}
for (i in seq) expr
```

Again, using curly braces is usually preferred, for loops can be nested and the indices need not be integers: 

```{r}
mat <- matrix(NA, nrow=3, ncol=3)
for (i in 1:3){
  for (j in 1:3){
    mat[i,j] <- paste(i, j, sep="-")
  }
} 
mat
```


While loops take the following form:
```{r, eval=FALSE}
while(cond) expr
```

```{r}
cumsum <- 0
while(cumsum <= 3) {
  cumsum <- cumsum + runif(1)
  print(cumsum)
}
```
A repeat loop is similar, but we must explicitly add a `break` to specify when to exit the loop:
```{r}
cumsum <- 0
repeat {
  cumsum <- cumsum + runif(1)
  print(cumsum)
  if (cumsum >= 3) break
}
```

Both of these constructs should be used with great care, as careless specification of the exiting condition can leave you stuck in an infinite loop. Try running the last example without the line specifying the break! Luckily RStudio allows you to interrupt such an endless loop using the little red stop button in the top right corner of the console window. 

## Vecotrisation  and `apply` family of funcitons

Looping functions - the `for()` loop in particular - are very intuitive and mastering them can represent a quick capability boost for a new R programmer. It is however highly recommended that you spend some time mastering the related `apply` family of functions, which should cover most of your looping needs. The genera rule is this: If you need to apply an expression over a series of elements and the order in which you do this is important, then use a loop. If the order is not important, take advantage of `apply`. In many circumstances this can improve the speed of your code, but in all cases it will make your code simpler and easier to read. 

The underlying logic of the `apply` family is starting out with some data structure (a vector, matrix, data.frame etc.), we want to split it into constituent parts, apply a function on each of them, and combine them back^[This idea comes from Hadley Wickham's paper on the split-apply-combine strategy of data analysis:  [http://vita.had.co.nz/papers/plyr.html](http://vita.had.co.nz/papers/plyr.html)]. We might for example want to apply a function on every row of a data.frame, every element of a vector, or every column in a matrix. 

### `apply()` 

The `apply()` function will apply a function to either the rows or the columns of a matrix. It's basic structure is: 

```{r, eval=FALSE}
apply(X, MARGIN, FUN, ...)
```
Where X is a matrix (if it is a data.frame, R will coerce it to a matrix), `MARGIN == 1` indicates rows, and `MARGIN == 2` indicates columns. A simple example of its use is to calculate row and column totals:

```{r}
mat <- matrix(1:9, 3,3)
# row totals
apply(mat, 1, sum)
# column totals
apply(mat, 2, sum)
```

In passing the function `FUN` in the example here we used built in function `sum`, but the real power of `apply` comes from integrating it with user defined functions. These are covered in the next section, but here is a quick example of how an in-line function can be used to find the second largest value in each row of a matrix. 



```{r}
mat <- matrix(sample(1:100, 25), 5,5)
mat
# find the second largest value in each row
apply(mat, 1, function(x) sort(x, decreasing = TRUE)[2])

# and for comparison, here is how we would do this using a for loop
out <- vector()
for (i in 1:nrow(mat)) {
  out[i] <- sort(mat[i,], decreasing = TRUE)[2]
}
out
```


### `lapply()` and `sapply()`

The functions `lapply()` and `sapply()` both apply a function to a vector, and the first returns a list back, while the second will try to simplify and return a vector. 


It is important to note that in R there are two types of vectors: i) atomic vectors and ii) lists. 

Furthermore, data frames in R are also represented as lists, with each column is an element of the list, represented by a vector. 

So this means both these functions can be applied to atomic vectors, to data frames, or to other types of lists:

```{r}
# a list of elements with different lengths:
test <- list(a = 1:5, b = 20:100, c = 17234) 
lapply(test, min)
sapply(test, min)

# a data frame (list of three vectors of equal length):
test <- data.frame(a = 1:5, b = 6:10, c = 11:15) 
lapply(test, mean)
sapply(test, mean)

# an atomic vector (this is rather silly, since sqrt(X) would work the same)
# but is added for completeness
test <- 1:3
lapply(test, sqrt)
sapply(test, sqrt)
```

By writing more elaborate functions and passing them as the argument to any of the `apply` family of functions, this seemingly simple construct can become incredibly powerful - as well as making the code eminently readable.


## Writing your own functions

One of the greatest strengths of R comes from writing your own functions. This not only allows you to repeat the same procedure consistently, but makes your code more structured and readable reduces chance of error, and will further strenghtens your reproducibility mentality. 

The basic construct is as follows:
```{r, eval = FALSE}
function.name <- function(arguments, ...) {
  expression
  (return value)
}
```

we have already seen the in-line version of the function call in the apply example above, remember:
```{r, eval = FALSE}
function(x) sort(x, decreasing = TRUE)[2]
```

Here our function takes a single argument (`x`), evaluates the expression (`sort(x, decreasing = TRUE)[2]`), and returns the value of that expression. This only works if the function has only a single expression, in which case the evaluated expression is returned. Otherwise we have to explicilty state what we want returned. We can rewrite this function in the more elaborate mode:


```{r}
FunSecondLargest <-function(x) {
  r <- sort(x, decreasing = TRUE)[2]
  return(r)
}
# now let's try it out with a sample vector
test.vector <- tidy.population2010$population

FunSecondLargest(test.vector)  
```

We can also now use this function directly in the apply call we used before:

```{r}
apply(mat, 1, FunSecondLargest)
```

We can also quickly rewrite the funciton to instead find the n-th largest value in the vector, by adding an additional argument `n`. And don't forget to write sensible comentary about what you are doing - at least for the benefit of your future self!

```{r}
# Function for extracting the n-th largest value from a vector
# Agruments:
#   x - vector 
#   n - optional integer value for rank
# Output:
# Returns single value 
FunNthLargest <-function(x, n=1) {
  r <- sort(x, decreasing = TRUE)[n]
  return(r)
}
# by default n=1, so it will find the largest value if we don't specify
FunNthLargest(test.vector)  
FunNthLargest(test.vector, n=2)  
FunNthLargest(test.vector, n=3)  
```

We could e.g. further generalise this function to look for the n-th smallest value, by adding another argument for the `TRUE/FALSE` value that gets passed to `decreasing` etc. Note unlike the argument `x`, the argument `n` has a default value (=1). This means we do not have to explicitly specify it unless we want it to be a different value. 

Functions have their own local environment, which is not accessible from the global environment. This means that whatever calculations are evaluated inside the function call do not clutter your workspace, but also their results are not accessible unles you explicilty return them from the function. Thus the object `r` will not be found in the global environment, instead we will get the error:
```{r, eval = FALSE}
r
Error: object 'r' not found
```

We can also have our funciton return several outputs for example:

```{r}
FunNthLargestElaborate <-function(x, n=1) {
  r <- sort(x, decreasing = TRUE)[n]
  desc <- paste("Rank", n, sep=":")
  return(c(desc, r))
}
FunNthLargestElaborate(test.vector, 3)
```

As we have seen before with if/else statements and loops, functions can also be nested -- as well as combined with if/else statements and loops! It is good practice to try to keep your code modular: keep your functions short and call them from eachother. This again makes it easier for the reader to understand what is going on, and easier for you to find errors or update your code. 

From a project management point of view it is also good practice to store all your functions in a separate file, which you `source()` at the begining of each session. You can even add `source("00-MyFunctions.R")` to your `.RProfile` file, which means all your bespoke functions will be automatically uploaded at the start of each session. 

\newpage
## PRACTICAL: If/else, loops, apply and functions
```{r}
## 3.1 Practice conditional expressions and logical operators 
###############################################################################
```
Practice conditional expressions and logical operators by seeing if you can figure out the results of the following expressions, then check them in R:
```{r}
x <- 1:7
y <- -3:3
x
y
```
```{r, eval=FALSE}
# try the following: 
(x == y)
(x > abs(y))
(x > 3) & (x < 5)
(x > 3) | (x < 5)
xor((x > 3), (x < 5))
(-1 %in% y)
(3 %in% y) & (3 %in% x) 
(3 %in% y) & !(3 %in% x) 
```

```{r, eval = FALSE}
## 3.2 Check consistency of tidy.economic.situation proportions ising IF - ELSE
###############################################################################
```
Use a `for()` loop to go through every row of `tidy.economic.situation` (tip: `nrow()` will tell you how many iterations you need):

* for each row add up the proportions for all three answers (columns two to four)
* use an `if/else` construct to check if the total equals 100
    + if it does, use `print` to print out an OK message
    + if it doesn't, print out a different message, one created using `paste` - so you can include the information on *which* row you have found the error.

```{r, eval = FALSE}
for (i in 1: nrow(tidy.economic.situation)) {
  if(???){
    ???} else {
      ???}
}
```

```{r, eval = FALSE}
## 3.3 Write a function to check consistency of tidy.economic.situation proportions
###############################################################################
```
Now write a function for the row checking you just did inside the `for()` loop. This is simply generalising the if/else expression to take a supplied argument instead of explicitly naming the row:

* Make sure you *document* your function correctly!
* the input for the function should be a row
* the output of the function should be  a variable called `message` - "OK" or "Not OK"
* Use the framework below:

```{r, eval = FALSE}
FunRowCheck <- function(x) {
  message <- if(???){
  } else {
  }
  return(message)
}
```

Now test out your function on a single row:
```{r, echo=11}
FunRowCheck <- function(x) {
  message <- if(sum(x) == 100){
    "OK"} else {
      "Not OK"}
  return(message)
}

## 3.4 Use your functions inside an apply statement
###############################################################################

# append the new test variable to the dataset
tidy.economic.situation$test <- apply(tidy.economic.situation[,2:4], 1, FunRowCheck)

FunRowCheck(tidy.economic.situation[1,2:4])
```

If it works correctly, you can now try using your new function inside an `apply` construct. 

Remember, `apply` will evaluate the expression along the *whole* row, and you want to apply it only to columns 2:4, so make sure you don't pass the whole table to `apply`. When you are happy with the result, append it to the table as an additional column:

```{r, eval = FALSE}
tidy.economic.situation$test <- apply(???)
```

Your table should now look like this:
```{r}
tidy.economic.situation
```

If it doesn't have a look at the `02-FunctionsAndLoops.R` file in the scripts folder for the solution.
If it does and you have plenty of time, you can try additionally:

* writing another function that will simply sum the three columns
* applying it to the table and appending the new variable (`total`)
* creating three new variables that are correctly scaled to sum up to 100 (e.g. `bad.scaled`, `good.sscaled`, `neutral.scaled`). 

```{r}
## 3.5 Bar-Plotting function
###############################################################################
```
To finish off this practical session we will write a function for plotting our table, using the `barplot()` function. This function only accepts vectors or matrices, but because our data is in a data.frame, we need to use `as.matrix()` for it to work, in addition to `t()` for transposing it. Here is the code for the most stripped down stacked barplot of the people's perceptions of their household financial situation. Note that the order of the columns had to be changed to make the more logical order of bad - neutral - good. 
```{r}
barplot(t(as.matrix(tidy.economic.situation[1:6,c(2,4,3)])))

```

Now expand the barplot function - use the help documentation in the help tab:

- add names to the x-axis
- add legend text
- add a main title
- feel free to explore additional arguments to the plot!

Once you are happy with your plot, enclose it in a function, so that you can pass it each of the three subsets of the table individually, and the function will additionally also change the plot's title to the correct one. 



\newpage
# Data manipulation with `dplyr`

Finally, the relatively new `dplyr` family of functions is one of the most powerful recent developments in the R coding world. While all of the data processing capabilities of `dplyr` existed in one shape or another in R before (usually several), `dplyr` brings them together in a comprehensive and systematic way, that allows for cleaner code that is easier to read, and faster to run. It also integrates logically with the `tidyr` family of functions described earlier, but it's most exciting and revolutionary aspect is it's assimilation of the *piping* or *chaining* of successive data processing functions - originally developed in the `magrittr` package, and now rightly becoming mainstream R practice. 

We will cover some of the most important functionalities of `dplyr` here, but you are encouraged to explore several excellent on-line resources, for example:

- The same [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) that also covers `tidyr`

- [Introduction vignette])https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)

- [dplyr and pipes: the basics](http://seananderson.ca/2014/09/13/dplyr-intro.html)


For this whole seciton we will be using the two data tables  prepared in the first practical. 

## Subsetting

### `filter()`
Extracts rows that meet the logical criteria:
```{r, eval = FALSE}
filter(data, criteria)
```

You can use any of the evaluation conditions and logical operators  we covered at the beginning of the previous section:

```{r, eval = FALSE}
filter(tidy.population2010, AREA_KM2 < 1000 & population > 10000 & AGE == 0)
```

### `select()`
Extracts only the columns that you list:
```{r, eval = FALSE}
select(data, list)
```

The columns can be named directly with no need for quotation marks
```{r, eval = FALSE}
select(tidy.economic.situation, bad, good, neutral)
```

In addition there is a large number of helper functions to select column names:

```{r}
# all the columns between bad and neutral
head(select(tidy.economic.situation,bad:neutral), n=3)
# all but the perception column
head(select(tidy.economic.situation, -perception), n=3)
# contains a dot in the name:
head(select(tidy.economic.situation, contains(".")), n=3)
# starts with the letter p
head(select(tidy.economic.situation, starts_with("p")), n=3)
# ends with the letter d
head(select(tidy.economic.situation, ends_with("d")), n=3)
# contains the text "n"
head(select(tidy.economic.situation, contains("n")), n=3)

```

You can also use select to reorder the columns, in our case we might want to reorder the three answer columns so:
```{r}
# change order of columns
head(select(tidy.economic.situation,income.group:bad, neutral, good:perception), n=3)
# we can also do this using the columns' respective indices instead
head(select(tidy.economic.situation, 1,2,4,3,5), n=3)
```

## Making new variables

New variables are easliy created using `mutate()`, which has the additional advantage of allowing you to reuse variables as you create them, without the need for an intermeiate step!
```{r}
# scale the values so they all sum up to 100
tidy.economic.situation <- mutate(tidy.economic.situation, total = bad+neutral+good, 
                                  bad.scaled = bad/total*100,
                                  good.scaled = good/total*100,
                                  neutral.scaled = neutral/total*100,
                                  total.scaled =  bad.scaled  + good.scaled + 
                                    neutral.scaled
)
head(tidy.economic.situation)
# we can now use select to remove the old ones
tidy.economic.situation <- select(tidy.economic.situation, -bad, - good, -neutral, 
                                  -total, -total.scaled)
# we could also use rename to rename the new ones
tidy.economic.situation <- rename(tidy.economic.situation, bad = bad.scaled, 
                                  good = good.scaled, neutral = neutral.scaled)
```


New variables can also be made using existing or user written functions. For example using `cut()` we can recode the bad variable into a categorical one:
```{r}
head(mutate(tidy.economic.situation, bad.cat = cut(bad, seq(0,100,10))))
```


## Summarizing
We can quickly summarise the data column wise using the `summarise()` function
```{r, eval = FALSE}
summarise(data, new.var = summary.function(column))
```

For example the average population, average area and total count in the population table, we can also use our own functions, such as the one we wrote before to get the second largest populaiton value
```{r}
summarise(tidy.population2010, pop = mean(population), area = mean(AREA_KM2), count = n(), 
          test = FunSecondLargest(population))
```

But the summarise funciton really comes into it's own when it operates on a *grouped* table. Using the function `group_by()` the table is (invisibly) split into subtables by the values of the grouping variable, and the summarise function then operates on each subset individually:
```{r}
summarise(group_by(tidy.population2010, AGE), 
          pop = mean(population), area = mean(AREA_KM2), count = n(), 
          test = FunSecondLargest(population))
```

An important corollary to the grouping function is `ungroup()`, which removes the grouping from the table for further analysis -- we shal use it in the last section of this chapter

## Joining tables
The `dplyr` pacgage also contains a set of functins that allow you to join tables by matching on common variables namely:
- left_join(a, b) 
- right_join(a, b) -- keeps all 
- inner_join(a, b) -- only keeps rows present in both a and b
- full_join(a, b) -- keeps all rows


```{r}
# prepare two small tables, one of UK men aged 0 or 1, the second of women aged 1 or 2:
UK.men <- filter(tidy.population2010, FIPS == "UK" & sex == "male" & (AGE == 0 | AGE == 1 ))
UK.women <- filter(tidy.population2010, FIPS == "UK", sex == "female" & (AGE == 1 | AGE == 2 ))

# try out all 4 merges on the two tables
left_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
right_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
inner_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
full_join(UK.men, UK.women, by = c("AGE", "NAME", "AREA_KM2", "FIPS"))
```

All of these have a `by=` argument, which lets you choose the columns to be joined by - if you do not explicitly name them, `dplyr` uses all the ones with identical names in both tables. If the columns you want to join by have different tables in each table you can specify this so: `by = c("name.a" = "name.b")`

### Other `dplyr` funcitons
The [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) is an indespensible help with `dplyr` funcitons. We will briefly mention a few more, but there are several more that we will not cover here.

`arrange()` for data sorting - ascending by default, otherwise specify `desc()`:
```{r}
arrange(tidy.economic.situation, perception, desc(bad))
```



## Piping/chaining daisies 

*Piping* data brings a completely new level of intuitiveness you R programming. Instead of nesting and indenting successive functions, which means the code has to be read *inside out*, piping (also known as daisy chaining) allows the code to be written in the natural direction in which the data is flowing.  The piping operator `%>%` indicates the direction of this flow as well, taking the output of the preceding function and directing it into the next one. 

Piping can be applied to almost any function, but shines particularly brightly when combining the dplyr functions we have just covered. For a pretty silly example:

```{r}
tidy.population2010 %>%
  filter(AREA_KM2 < 2000 & population > 15000 & AGE == 0) %>%
  select(-AGE) %>%
  mutate(density = population/AREA_KM2) %>%
  group_by(NAME) %>%
  summarise(count = n(), mean.density = mean(density)) 
```

You will note that we skipped naming the data object in all of functions. The piping operator means it is implicit what the data being passed on is, so there is no more need to explixitly name it. 

In keeping with the piping logic, we can also use a rarely used R assignment operator: `->`. We can add it at the end of the pipe/chain and point it to the new object's name. Of course you can also start the way we have been starting all along, and assing in the standard direction `<-` if you prefer. 

A whole set of piped functions can easily be wrapped up in a function: 
```{r}
FunMyPipe <- function(x) {
  x %>% 
    sqrt %>%   # square root
    mean %>%   # mean  
    "*" (100)  # multiplication - a bit awkward, true
}
# Test it out on a short vector
FunMyPipe(1:10)
```


## PRACTICAL 
```{r}
## 4.1 Write a function to extract population pyramid data
###############################################################################
```
Write a function that uses piping and `dplyr` functions to do the following:

* the input is the FIPS country code [https://en.wikipedia.org/wiki/List_of_FIPS_country_codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes)
* from  tidy.population2010 extracts the data for that country
* remove variables for the area
* create a new variable grouping the ages into 5 year age groups
* find the sum of the population for each age group and gender combination (use `group_by`!)
* don't forget to `ungroup` the data before the next step!
* create a new variable representing the proportion of the total population in each age/sex combination (* 100)
* return this table, which should have 40 rows and 4 columns.

```{r}
## 4.2 Write a function to draw a population pyramid plot
###############################################################################
```
We will use the `plotrix` package for this, although you are free to experiment with drawing your own pyramid plot - you will have much more control than using the default `pyramid.plot()` function. Have a look at the help documentation for this function. In particular note that you need to provide it two vecotrs, `lx` and `rx` for the popoulation sizes. 

Write a function that:

* takes as it's input the output from your previous function (the 40x4 table)
* creates the `lx` and `rx vectors
* **IMPORTANT** again you will have a data.frame as the result. use `as.matrix` on `lx` and `rx` so they can be used in the plot
* calls pyramid.plot(lx, rx) as well as any other arguments you may want to add. (in particular you may want to add `labels=`). One way of creating them is `paste(seq(0,96, 5), seq(5,100,5), sep="-")`, but you can also use the values from the age group vairable. 



```{r, echo = FALSE}
FunPopClean <- function(cntry = "UK"){
   tidy.population2010 %>%
    filter(FIPS == cntry) %>%
    mutate(age.g = cut(AGE, 20)) %>%
    group_by(age.g, sex) %>%
    summarise(population = sum(population)) %>%
    ungroup() %>%
    mutate(prop = 100*population/sum(population)) ->
    data
  return(data)
}

FunPlot <- function(data, name = NULL){
  data %>%  
    filter( sex=="male") %>%
    select(prop) %>% 
    as.matrix -> lx
  data %>%  
    filter( sex=="male") %>%
    select(prop) %>% 
    as.matrix -> rx
  pyramid.plot(lx, rx,
               main=name,
               labels = paste(seq(0,96, 5), seq(5,100,5), sep="-"))
}
```
\newpage
```{r fig.width = 6, fig.height = 5 }
FunPlot(FunPopClean("BG"), "Bangladesh")  
  
```


